{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6b5ee2",
   "metadata": {},
   "source": [
    "# 모듈은 첫번째 문단에 모두 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e33c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import re\n",
    "import difflib\n",
    "import os\n",
    "import shutil\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660bb54",
   "metadata": {},
   "source": [
    "- 디렉토리를 변경하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a0d2868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi welcome\n",
      "We are here  C:\\Users\\Sang Duck Park\\Desktop\\Mol-smi\\901-1000\n",
      "Check the directory and change it, If not\n",
      "C:\\Users\\Sang Duck Park\\Desktop\\Mol-smi\\mol\n",
      "C:\\Users\\Sang Duck Park\\Desktop\\Mol-smi\\mol\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "print(\"Hi welcome\")\n",
    "print(\"We are here \",os.getcwd())\n",
    "print(\"Check the directory and change it, If not\")\n",
    "f_directory = input()\n",
    "if f_directory == '0':\n",
    "    f_directory = os.getcwd()\n",
    "try:\n",
    "    os.chdir(f_directory)\n",
    "    print(os.getcwd())\n",
    "    print(\"success\")\n",
    "except:\n",
    "    print(\"check the directory again\")\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858c56d",
   "metadata": {},
   "source": [
    "- smi 파일내에 있는 *을 바꿔주는 프로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b02ff625",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in smi_list:\n",
    "    with open(ind, \"r\") as f:\n",
    "        line = f.readline()\n",
    "    line = line.replace('*','')\n",
    "    end_point = line.find('\\t')\n",
    "    new_line = line[:end_point]\n",
    "    with open(ind, \"w\") as f:\n",
    "         f.write(new_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7238bcc1",
   "metadata": {},
   "source": [
    "- mask 와 where 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟데이터의 이름을 리스트로 만들어준 후 범위 내에 존재하는지 검색한다\n",
    "name_list = list(target_df['NAME'])\n",
    "#마스크를 씌우면 존재하지 않는 존재만 남게된다.\n",
    "isin_filter = range_df['Material'].isin(name_list)\n",
    "#이름이 없는 범위 데이터 프레임\n",
    "Wrong_df = range_df.mask(isin_filter)\n",
    "Wrong_df.dropna(subset=[\"Material\"],inplace = True)\n",
    "#이름이 있는  범위 데이터 프레임\n",
    "Match_df = range_df.drop(list(Wrong_df.index))\n",
    "exist_name = list(Match_df['Material'])\n",
    "#범위 데이터 프레임과 타겟 프레임이 일치하지 않는 데이터 프레임\n",
    "target_filter = target_df['NAME'].isin(exist_name)\n",
    "rest_df = target_df.mask(target_filter)\n",
    "rest_df.dropna(subset=[\"NAME\"],inplace = True)\n",
    "# 범위 데이터 프레임과 타겟 프레임이 일치하는 데이터 프레임\n",
    "m_df = target_df.drop(list(rest_df.index))\n",
    "m_df.dropna(subset=[\"NAME\"],inplace = True)\n",
    "#남은 데이터와 일치하는 데이터가 필요하다.\n",
    "rest_name = list(rest_df['NAME'])\n",
    "#일치 하지 않은 이름 리스트에서 찾아야한다.\n",
    "N_match_name = list(Wrong_df[\"Material\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273966e",
   "metadata": {},
   "source": [
    "- 파일 이동 및 카피 코드 (material_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ac40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_name(name):\n",
    "    name = name.split(\".\")[-1]\n",
    "    name = name.split(\"_\")\n",
    "    first_name = name[0]\n",
    "    second_name = name[1]\n",
    "    return first_name ,second_name\n",
    "\n",
    "path = \"C:/Users/Sang Duck Park/Desktop/Material_project/Poscar/\"\n",
    "file_list = os.listdir(path)\n",
    "new_path = 'C:/Users/Sang Duck Park/Desktop/Material_project/order_file/'\n",
    "if not os.path.exists(new_path):\n",
    "    os.mkdir(new_path)\n",
    "    \n",
    "cnt = 1\n",
    "data_name = list()\n",
    "for name in file_list2:\n",
    "    r_name , c_name = re_name(name)\n",
    "    file = str(cnt)+\"_\"+r_name+\".vasp\"\n",
    "    shutil.copy(path + name, new_path + file)\n",
    "    data_name.append([cnt,file,r_name,c_name])\n",
    "    cnt += 1\n",
    "\n",
    "# 데이터 프레임 만드는 법\n",
    "list_df = pandas.DataFrame(data_name, columns=['num','file','ID','compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d60b6c",
   "metadata": {},
   "source": [
    "- 높은 값만 찾아주는 프로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a173ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZT_df = df[['elements', 'ZT']]\n",
    "MAX_ZT = []\n",
    "cnt = 0\n",
    "for index in ZT_df['elements']:\n",
    "    check = 0\n",
    "    num = cnt + 2\n",
    "    ZT = float(ZT_df.iloc[cnt]['ZT'])\n",
    "    if cnt == 0:\n",
    "            MAX_ZT.append([index,ZT,num])\n",
    "            cnt += 1\n",
    "            continue\n",
    "    for comp in MAX_ZT:\n",
    "        if index == comp[0]:\n",
    "            if ZT > comp[1]:\n",
    "                comp[1] = ZT\n",
    "                comp[2] = num\n",
    "            check = 1\n",
    "            break\n",
    "        if check == 0:\n",
    "            MAX_ZT.append([index,ZT,num])\n",
    "        cnt += 1\n",
    "\n",
    "highest_ZT=pd.DataFrame(MAX_ZT)\n",
    "highest_ZT.to_excel('highest_ZT.xlsx', index=False)\n",
    "print(\"A file named highest_ZT.xlsx has been generated.\\nPlease check your folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f88fe",
   "metadata": {},
   "source": [
    "- 원소 분해 후 전체 갯수 세는 프로그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32316fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'highest_ZT.xlsx')   \n",
    "df.head()\n",
    "df.dtypes\n",
    "\n",
    "history = []\n",
    "\n",
    "for comp in df['Composition']:\n",
    "    list1 = mg.Composition(comp).elements\n",
    "    for index in list1:\n",
    "        valid = 0\n",
    "        for his in history:\n",
    "            if his[0] == index:\n",
    "                his[1] = his[1] + 1\n",
    "                valid = 1\n",
    "                break\n",
    "        if valid == 0:\n",
    "            history.append([index,1])\n",
    "            \n",
    "History=pd.DataFrame(history)\n",
    "History.to_excel('History.xlsx', index=False)\n",
    "print(\"A file named History.xlsx has been generated.\\nPlease check your folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed6a86b",
   "metadata": {},
   "source": [
    "- unrelaxed structure cif to vasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7843e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "\n",
    "url = 'https://www.cryst.ehu.es/cgi-bin/cryst/programs/mcif2vesta/index.php'\n",
    "files_path = \"C:\\\\Users\\\\Sang Duck Park\\\\Desktop\\\\00a5c7beab012896.cif\"\n",
    "\n",
    "#browser = webdriver.Firefox()\n",
    "browser = webdriver.Chrome()\n",
    " \n",
    "browser.get(url)\n",
    "\n",
    "#upload = browser.find_element_by_xpath('/html/body/table[2]/tbody/tr/td[2]/form[1]/table/tbody/tr[1]/td/input[2]')\n",
    "upload = browser.find_element_by_xpath('/html/body/table[2]/tbody/tr/td[2]/form[1]/table/tbody/tr[1]/td/input[1]')\n",
    "upload.send_keys(files_path)\n",
    "submit = browser.find_element_by_xpath('/html/body/table[2]/tbody/tr/td[2]/form[1]/table/tbody/tr[1]/td/input[2]')\n",
    "submit.click()\n",
    "\n",
    "WebDriverWait(browser, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"head\")))\n",
    "\n",
    "space_group = browser.find_element_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/form/p[2]/table[1]/tbody/tr[1]/td[2]/input\")\n",
    "space_group.clear()\n",
    "space_group.send_keys('55')\n",
    "export_vasp = browser.find_element_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/form/center/input[5]\")\n",
    "export_vasp.click()\n",
    "\n",
    "WebDriverWait(browser, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"body > a:nth-child(10)\")))\n",
    "\n",
    "\n",
    "download = browser.find_element_by_xpath(\"/html/body/a[2]\")\n",
    "download.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32ec41",
   "metadata": {},
   "source": [
    "- Material Project 크롤링도구\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebd991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "URL = 'https://materialsproject.org/materials/'\n",
    "browser = webdriver.Chrome()\n",
    "#타겟 지정 버튼 작업 및 다운로드\n",
    "\n",
    "def Download_file(f_name):\n",
    "    url = URL + str(f_name)\n",
    "    browser.get(url)\n",
    "    r_time = random.randrange(1,3)\n",
    "    try:\n",
    "        WebDriverWait(browser, 20).until(EC.presence_of_element_located((By.CSS_SELECTOR,'#material > div:nth-child(4) > div.span9 > div:nth-child(2) > div.span6 > div.row.pull-down > div > div > button')))\n",
    "        browser.find_element_by_css_selector('#material > div:nth-child(4) > div.span9 > div:nth-child(2) > div.span6 > div.row.pull-down > div > div > button').click()\n",
    "        time.sleep(r_time)\n",
    "        browser.find_element_by_css_selector('#material > div:nth-child(4) > div.span9 > div:nth-child(2) > div.span6 > div.row.pull-down > div > div > ul > li:nth-child(3)').click()\n",
    "        browser.find_element_by_xpath('//*[@id=\"download-files\"]').click()\n",
    "        return True\n",
    "    except :\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "#URL 사전 작업\n",
    "    data_df = pd.read_csv(\"2Dmatpedia (1).csv\")\n",
    "    false_list = list()\n",
    "\n",
    "    for f_name in data_df['Obtained from']:\n",
    "        switch = Download_file(f_name)\n",
    "        if not switch:\n",
    "            false_list.append(f_name)\n",
    "\n",
    "        r_time = random.randrange(4,7)\n",
    "        time.sleep(r_time)\n",
    "\n",
    "    with open(\"false_list\", 'w+') as lf:\n",
    "        lf.write('\\n'.join(testList))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a327a",
   "metadata": {},
   "source": [
    "- Crolling cif file to aflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "from urllib.request import urlretrieve\n",
    "import urllib.request\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from time import sleep\n",
    "\n",
    "def download(url,ids):        \n",
    "        try:\n",
    "                file_name = url.split('/')[-1] +'.cif'\n",
    "                durl = \"http://\"+url.replace(\":\",\"/\") + \"/\" + file_name\n",
    "                urlretrieve(durl, ids)\n",
    "                return None\n",
    "        except:\n",
    "                download_2(url,ids)\n",
    "                \n",
    "\n",
    "        \n",
    "def download_2(url,ids):\n",
    "        try:\n",
    "                name = url.split('/')\n",
    "                file_name = '.'.join(name[-2:]) + \".cif\"\n",
    "                some = \"http://\"+url.replace(\":\",\"/\") + \"/\" + file_name\n",
    "                urlretrieve(some, ids)\n",
    "                return None\n",
    "        except:\n",
    "                download_3(url,ids)\n",
    "\n",
    "def download_3(url,ids):\n",
    "        try:\n",
    "                name = url.split('/')\n",
    "                file_name = '.'.join(name[-3:-1]).replace(\":OLD\",\"\") +':' + name[-1]+ \".cif\"\n",
    "                some = 'http://aflowlib.duke.edu' + \"/\" + \":\".join(url.split(':')[1:])  + \"/\" + file_name\n",
    "                urlretrieve(some,ids)\n",
    "                return None\n",
    "        except:\n",
    "                download_4(url,ids)\n",
    "                \n",
    "\n",
    "def download_4(url,ids):\n",
    "        try:\n",
    "                name = url.split('/')\n",
    "                file_name = '.'.join(name[-3:-1]) + \".cif\"\n",
    "                some = 'http://aflowlib.duke.edu' + \"/\" + \":\".join(url.split(':')[1:]) + \"/\" + file_name\n",
    "                urlretrieve(some, ids)\n",
    "                return None\n",
    "        \n",
    "        except:\n",
    "                print(url)\n",
    "                return 1\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "        url = 'http://www.aflowlib.org/API/aflux/?compound,$paging(0)'        \n",
    "        response = urllib.request.urlopen(url)\n",
    "        response_message = response.read().decode('utf8')\n",
    "        data = json.loads(response_message)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "        df.to_csv(\"Dataset.csv\")\n",
    "\n",
    "        error_list = list()\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "                ids = row[\"auid\"].split(\":\")[-1] + \".cif\"\n",
    "                error = download(row[\"aurl\"],ids)\n",
    "                if error:\n",
    "                        error_list.append(index)\n",
    "                        \n",
    "                sleep(0.05)\n",
    "\n",
    "\n",
    "\n",
    "        with open('errors.txt', 'w') as f:\n",
    "                f.writelines(error_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769dc9e",
   "metadata": {},
   "source": [
    "- making structure table code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71fc681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from html_table_parser import parser_functions as parser\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def transform(word):\n",
    "        word = re.sub('<[a,i].*?>',\"\",word)\n",
    "        word = re.sub('</[a,i]>',\"\",word)\n",
    "        if '<sub>' in word:\n",
    "                word = re.sub('<sub>',\"[\",word)\n",
    "                word = re.sub('</sub>',\"]\",word)\n",
    "        return word\n",
    "                \n",
    "        \n",
    "        \n",
    "\n",
    "                \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "        url = 'https://www.cryst.ehu.es/cgi-bin/cryst/programs/nph-getgen/'        \n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            table = soup.find('table', {'border': '0'})\n",
    "            row_html = table.find_all('tr')\n",
    "            data_table = list()\n",
    "            for row in row_html:\n",
    "                    data = row.find_all('td')\n",
    "                    for d in data:\n",
    "                            if d['align'] == 'right':\n",
    "                                    pair_list = [d.text]\n",
    "                            else:\n",
    "                                    word = str(d.find(\"a\"))\n",
    "                                    pre_data = transform(word)\n",
    "                                    pair_list.append(pre_data)\n",
    "                                    data_table.append(pair_list)\n",
    "            df = pd.DataFrame(data_table, columns = ['Number','Symbols']) \n",
    "                \n",
    "                    \n",
    "            #table = parser.make2d(data)\n",
    "            #df = pd.DataFrame(data=table[1:])\n",
    "\n",
    "        else : \n",
    "            print(response.status_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
